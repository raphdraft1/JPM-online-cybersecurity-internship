{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df29eda",
   "metadata": {
    "id": "1df29eda"
   },
   "source": [
    "Step 0. Unzip enron1.zip into the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf32cfce",
   "metadata": {
    "id": "bf32cfce"
   },
   "source": [
    "Step 1. Traverse the dataset and create a Pandas dataframe. This is already done for you and should run without any errors. You should recognize Pandas from task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20c5d195",
   "metadata": {
    "id": "20c5d195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped 2248.2004-09-23.GP.spam.txt\n",
      "skipped 2526.2004-10-17.GP.spam.txt\n",
      "skipped 2698.2004-10-31.GP.spam.txt\n",
      "skipped 4566.2005-05-24.GP.spam.txt\n",
      "Number of Ham Emails: 3672\n",
      "Number of Spam Emails: 1496\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_spam():\n",
    "    category = 'spam'\n",
    "    directory = './enron1/spam'\n",
    "    return read_category(category, directory)\n",
    "\n",
    "def read_ham():\n",
    "    category = 'ham'\n",
    "    directory = './enron1/ham'\n",
    "    return read_category(category, directory)\n",
    "\n",
    "def read_category(category, directory):\n",
    "    emails = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        with open(os.path.join(directory, filename), 'r') as fp:\n",
    "            try:\n",
    "                content = fp.read()\n",
    "                emails.append({'name': filename, 'content': content, 'category': category})\n",
    "            except:\n",
    "                print(f'skipped {filename}')\n",
    "    return emails\n",
    "\n",
    "ham_emails = read_ham()\n",
    "spam_emails = read_spam()\n",
    "\n",
    "ham_df = pd.DataFrame.from_records(ham_emails)\n",
    "spam_df = pd.DataFrame.from_records(spam_emails)\n",
    "\n",
    "print(\"Number of Ham Emails:\", len(ham_df))\n",
    "print(\"Number of Spam Emails:\", len(spam_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c23fd",
   "metadata": {
    "id": "1a1c23fd"
   },
   "source": [
    "Step 2. Data cleaning is a critical part of machine learning. You and I can recognize that 'Hello' and 'hello' are the same word but a machine does not know this a priori. Therefore, we can 'help' the machine by conducting such normalization steps for it. Write a function `preprocessor` that takes in a string and replaces all non alphabet characters with a space and then lowercases the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c447c901",
   "metadata": {
    "id": "c447c901"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(e):\n",
    "    processed_text = re.sub(r'[^a-zA-Z]', ' ', e)\n",
    "    processed_text = processed_text.lower()\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32521d",
   "metadata": {
    "id": "ba32521d"
   },
   "source": [
    "Step 3. We will now train the machine learning model. All the functions that you will need are imported for you. The instructions explain how the work and hint at which functions to use. You will likely need to refer to the scikit learn documentation to see how exactly to invoke the functions. It will be handy to keep that tab open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1442d377",
   "metadata": {
    "id": "1442d377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.9748792270531401\n",
      "\n",
      "Confusion Matrix:\n",
      "[[717  18]\n",
      " [  8 292]]\n",
      "\n",
      "Detailed Statistics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.98      0.98       735\n",
      "        spam       0.94      0.97      0.96       300\n",
      "\n",
      "    accuracy                           0.97      1035\n",
      "   macro avg       0.97      0.97      0.97      1035\n",
      "weighted avg       0.98      0.97      0.97      1035\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rapha\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Instantiate CountVectorizer\n",
    "count_vectorizer = CountVectorizer(preprocessor=preprocessor)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train_ham, X_test_ham, y_train_ham, y_test_ham = train_test_split(ham_df[\"content\"], ham_df[\"category\"], test_size=0.2, random_state=1)\n",
    "X_train_spam, X_test_spam, y_train_spam, y_test_spam = train_test_split(spam_df[\"content\"], spam_df[\"category\"], test_size=0.2, random_state=1)\n",
    "\n",
    "# Concatenate the training and testing sets for both ham and spam\n",
    "X_train = pd.concat([X_train_ham, X_train_spam])\n",
    "X_test = pd.concat([X_test_ham, X_test_spam])\n",
    "y_train = pd.concat([y_train_ham, y_train_spam])\n",
    "y_test = pd.concat([y_test_ham, y_test_spam])\n",
    "\n",
    "# Transform the dataset using CountVectorizer\n",
    "X_train_transformed = count_vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Fit Logistic Regression model to the train dataset\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Generate predictions on the test dataset\n",
    "y_pred = logreg.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Accuracy:\\n{accuracy_score(y_test, y_pred)}\\n')\n",
    "print(f'Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\\n')\n",
    "print(f'Detailed Statistics:\\n{classification_report(y_test, y_pred)}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9674d032",
   "metadata": {
    "id": "9674d032"
   },
   "source": [
    "Step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b7d78c9",
   "metadata": {
    "id": "6b7d78c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 positive features (associated with spam):\n",
      "0.9053617461034007 quzqqcy\n",
      "0.8530954395077608 toiling\n",
      "0.8310957525690819 wncl\n",
      "0.7752318571570987 synchronous\n",
      "0.7501530396859558 levar\n",
      "0.7160693884730204 acronymbronchitis\n",
      "0.6903139675750463 vai\n",
      "0.6809317580870108 limitate\n",
      "0.652019209528551 fullness\n",
      "0.6472422612668653 ikeja\n",
      "\n",
      "Top 10 negative features (associated with ham):\n",
      "-1.805572070184771 caught\n",
      "-1.542090920102343 chawkins\n",
      "-1.4784043515773455 tified\n",
      "-1.3603392075670555 smerrill\n",
      "-1.3446254573683243 bd\n",
      "-1.172531720742926 dugout\n",
      "-1.1508078100744572 dime\n",
      "-1.081884981629174 czwsxm\n",
      "-1.078875815265842 archilochian\n",
      "-1.044061936205263 utdallas\n"
     ]
    }
   ],
   "source": [
    "# Let's see which features (aka columns) the vectorizer created. \n",
    "# They should be all the words that were contained in the training dataset.\n",
    "# Get the vocabulary (terms) from the CountVectorizer object\n",
    "vocabulary = count_vectorizer.vocabulary_\n",
    "\n",
    "# Get the feature names (words) from the vocabulary\n",
    "features = list(vocabulary.keys())\n",
    "\n",
    "# You may be wondering what a machine learning model is tangibly. It is just a collection of numbers. \n",
    "# You can access these numbers known as \"coefficients\" from the coef_ property of the model\n",
    "# We will be looking at coef_[0] which represents the importance of each feature.\n",
    "# What does importance mean in this context?\n",
    "# Some words are more important than others for the model.\n",
    "# It's nothing personal, just that spam emails tend to contain some words more frequently.\n",
    "# This indicates to the model that having that word would make a new email more likely to be spam.\n",
    "importance = logreg.coef_[0]\n",
    "\n",
    "# Iterate over importance and find the top 10 positive features with the largest magnitude.\n",
    "# Similarly, find the top 10 negative features with the largest magnitude.\n",
    "# Positive features correspond to spam. Negative features correspond to ham.\n",
    "# You will see that `http` is the strongest feature that corresponds to spam emails. \n",
    "# It makes sense. Spam emails often want you to click on a link.\n",
    "# Create a list of tuples containing index and importance\n",
    "indexed_importance = list(enumerate(importance))\n",
    "\n",
    "# Sort the list based on importance (descending order)\n",
    "indexed_importance.sort(key=lambda e: e[1], reverse=True)\n",
    "\n",
    "# Print the top 10 features with highest importance\n",
    "print(\"Top 10 positive features (associated with spam):\")\n",
    "for i, imp in indexed_importance[:10]:\n",
    "    print(imp, feature_names[i])\n",
    "\n",
    "# Sort the list in reverse order to get top negative features\n",
    "indexed_importance.sort(key=lambda e: -e[1], reverse=True)\n",
    "\n",
    "# Print the top 10 features with lowest importance (negative)\n",
    "print(\"\\nTop 10 negative features (associated with ham):\")\n",
    "for i, imp in indexed_importance[:10]:\n",
    "    print(imp, feature_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d267e7ad",
   "metadata": {
    "id": "d267e7ad"
   },
   "source": [
    "Submission\n",
    "1. Upload the jupyter notebook to Forage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LI4u_ZUGToDQ",
   "metadata": {
    "id": "LI4u_ZUGToDQ"
   },
   "source": [
    "All Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fef61db-d5d6-41f8-b1f0-4508a14292c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
